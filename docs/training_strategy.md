# 学習戦略

ここでは GD-高速道路ゴシックJA の欠損グリフを高品質に生成するための学習方針をまとめます。

## 1. 2 段階学習

1. **事前学習**: まず既存の全グリフペアを用いて通常の学習率 (例: `2e-4`) で 200〜300 エポック学習します。これによりストローク変換の基礎を獲得します。
2. **微調整**: 次に生成精度の低い文字や不足文字のみを抽出し、学習率を下げて (例: `1e-5`) 20〜50 エポック程度追加学習します。基礎モデルを壊さず細部だけを強化できます。

`train_pix2pix.py` では `stagewise_train` 関数でこの流れを実行できます。
Stage2 では学習率を大幅に下げ、必要に応じてジェネレータの前半層を凍結することで Catastrophic Forgetting を防ぎます。さらに既存文字の一部を混合するリハーサルにも対応しました。

## 2. データ増強

学習画像枚数が少ない場合は以下の前処理を組み合わせてロバスト性を高めます。

- 小さなアフィン変換 (平行移動 ±5px、回転 ±2°、スケーリング ±5%)
- 軽いガウシアンノイズの付与
- ランダムな太さ変更 (膨張／収縮)
- 参考フォントのみへのぼかし・アフィン変換
- ターゲット側には軽度の太さ変化のみ適用

データセットの偏りが大きい場合は、Stage2 で `rehearsal_ratio` を調整して既存文字
を混合し、バランスを保つと効果的です。

## 3. 追加損失の検討

視覚的一貫性向上のために perceptual loss などの特徴量損失を導入することも可能です。従来は L1 損失と GAN 損失のみでしたが、VGG16 特徴を用いた Perceptual Loss を追加し、細部の再現性を高めます。重みは `perceptual_lambda` で調整できます。

## 4. 高解像度学習

モデルは 256px 正方形画像を前提としていますが、U-Net の段数を調整すれば 512px 以上でも学習可能です。制作環境で 1000x1000 の座標系と太さ 90 を用いる場合は、512px 以上の入力サイズでの学習を検討するとより精細な出力が期待できます。
これらの手法を組み合わせ、段階的にチューニングすることで少量データでも安定したグリフ生成が期待できます。

## 5. 定量評価と学習率スケジュール

学習過程では `train_pix2pix.py` が PSNR と SSIM を計算し、出力品質をモニタリングします。
また `CosineAnnealingLR` を用いて学習率を徐々に減衰させることで、過学習を抑えつつ安定した収束を図ります。
`log_memory` オプションを利用すると、エポックごとの GPU メモリ消費量も確認できます。

## 6. 検証データとベストモデル

学習用データを 9:1 などの比率で分割し、一部を検証用に回すことで過学習を監視できます。エポック終了時に検証データで PSNR/SSIM を測定し、最高値を更新した場合はモデルを `G_best.pth` として保存します。長時間の学習でも最も良い結果だけを簡単に取得できます。

